"""
Script de teste para a URL espec√≠fica que est√° faltando 1 im√≥vel
"""

from zap_scraper import ZapScraper
import time

def testar_url_especifica():
    """Testa a URL espec√≠fica que est√° com problema"""
    
    # URL que est√° faltando 1 im√≥vel (deveria ter 3, mas s√≥ encontra 2)
    url_problema = "https://www.zapimoveis.com.br/lancamentos/apartamentos/sp+sao-paulo/pronto-para-morar/?transacao=lancamentos&onde=%2CS%C3%A3o+Paulo%2CS%C3%A3o+Paulo%2C%2C%2C%2C%2Ccity%2CBR%3ESao_Paulo%3ENULL%3ESao_Paulo%2C-23.555771%2C-46.639557%2C&tipos=apartamento_residencial&quartos=2%2C3%2C4&precoMaximo=300000&tipo=Pronto+para+morar"
    
    print("üîç Testando URL espec√≠fica com problema...")
    print(f"URL: {url_problema}")
    print("=" * 80)
    
    # Criar inst√¢ncia do scraper
    scraper = ZapScraper()
    
    try:
        print("üöÄ Iniciando teste...")
        
        # Configurar driver
        if not scraper.configure_driver():
            print("‚ùå Erro ao configurar driver")
            return
        
        print("‚úÖ Driver configurado com sucesso")
        
        # Acessar p√°gina
        scraper.driver.get(url_problema)
        time.sleep(5)  # Aguardar carregamento inicial
        
        print("‚úÖ P√°gina acessada")
        
        # Salvar HTML inicial para debug
        scraper.debug_salvar_html("debug_inicial.html")
        
        # Aguardar elementos carregarem
        from selenium.webdriver.common.by import By
        from selenium.webdriver.support.ui import WebDriverWait
        from selenium.webdriver.support import expected_conditions as EC
        
        try:
            WebDriverWait(scraper.driver, 20).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, "[data-cy*='property']"))
            )
            print("‚úÖ Elementos de propriedade detectados")
        except:
            print("‚ö†Ô∏è Timeout aguardando elementos de propriedade")
        
        # Fazer scroll
        print("\nüìú Fazendo scroll...")
        num_elementos = scraper.scroll_page()
        print(f"‚úÖ Scroll conclu√≠do. Elementos encontrados: {num_elementos}")
        
        # Salvar HTML ap√≥s scroll
        scraper.debug_salvar_html("debug_apos_scroll.html")
        
        # Testar diferentes seletores
        print("\nüîç Testando diferentes seletores...")
        seletores_teste = [
            "flex.flex-col.grow.min-w-0",
            "[data-cy*='property-card']",
            "[data-testid*='property']",
            "[class*='property-card']",
            ".card-property",
            "[data-cy*='lancamento']",
            "[data-testid*='lancamento']",
            ".lancamento-card",
            "[class*='lancamento']"
        ]
        
        for seletor in seletores_teste:
            try:
                elementos = scraper.driver.find_elements(By.CSS_SELECTOR, seletor)
                print(f"Seletor '{seletor}': {len(elementos)} elementos")
                
                if elementos:
                    # Mostrar informa√ß√µes dos primeiros elementos
                    for i, elemento in enumerate(elementos[:3]):
                        try:
                            texto = elemento.text.strip()[:100]
                            print(f"  Elemento {i+1}: {texto}...")
                        except:
                            print(f"  Elemento {i+1}: [Erro ao obter texto]")
            except Exception as e:
                print(f"Seletor '{seletor}': Erro - {e}")
        
        # Tentar extrair dados
        print("\nüìä Tentando extrair dados...")
        dados = scraper.extrair_dados_pagina(url_problema, max_paginas=1)
        
        if dados is not None and not dados.empty:
            print(f"‚úÖ Dados extra√≠dos: {len(dados)} im√≥veis")
            print("\nüìã Resumo dos dados:")
            print(dados.to_string())
            
            # Salvar dados
            # Criar pasta arquivos se n√£o existir
            pasta_arquivos = "arquivos"
            if not os.path.exists(pasta_arquivos):
                os.makedirs(pasta_arquivos)
            
            caminho_csv = os.path.join(pasta_arquivos, "teste_url_especifica.csv")
            dados.to_csv(caminho_csv, index=False)
            print("\nüíæ Dados salvos em 'teste_url_especifica.csv'")
        else:
            print("‚ùå Nenhum dado foi extra√≠do")
        
        # Salvar HTML final
        scraper.debug_salvar_html("debug_final.html")
        
    except Exception as e:
        print(f"‚ùå Erro durante o teste: {e}")
        
    finally:
        try:
            if scraper.driver:
                scraper.driver.quit()
        except:
            pass

if __name__ == "__main__":
    testar_url_especifica()
